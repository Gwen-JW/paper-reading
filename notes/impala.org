#+TITLE: IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures
#+AUTHOR: fangyuan


IMPALA uses an actor-critic setup to learn a policy $\pi$ and a baseline
function $V^{\pi}$. Like [[dper.org][DPER]], the process of generating experiences is
decoupled from learning the parameters. The architecture consists of a
set of actors, repeatedly generating trajectories of experience, and one
or more learners that use the experiences sent from actors to learn $\pi$
off-policy.

This simple architecture enables the learners to be accelerated using GPUs
and actors to be easily distributed across many machines. However, the
learner policy $\pi$ is potentially several updates ahead of the actor's
policy $\mu$ at the time of update, therefore there is a ~policy-lay~ between
the actors and learners. So they use V-trace corrects this lag.

** V-trace
V-trace is actually a off-policy GAE(generalized advantage estimate).

In on-policy RL, the n-steps Bellman target is
\begin{array}
v_s &= V(x_s) + \sum_{t=s}^{s+n-1}\gamma^{t-s}(r_t + \gamma V(x_{t+1}) - V(x_t)) \\
&= \sum_{t=s}^{s+n-1}\gamma^{t-s}r_t + \gamma^n V(x_{s+n})
\end{array}

However, in off-policy RL, they use Importance Sampling to correct this
\begin{array}
v_s &= V(x_s) + \sum_{t=s}^{s+n-1}\gamma^{t-s} \prod_{i=s}^{t-1}\frac{\pi(a_i|x_i)}{\mu(a_i|x_i)} (r_t + \gamma V(x_{t+1}) - V(x_t)) \\
\end{array}

They use the truncated IS as a variance reduction technique
\begin{equation*}
c_i = \min(\bar{c}, \frac{\pi(a_i|x_i)}{\mu(a_i|x_i)})
\end{equation*}
The more dissimilar $\pi$ and $\mu$ are (the more off-policy we are), the
larger the variance of this product.

And there's also weighted temporal difference $\delta_t V$ by $\rho_t$.
\begin{equation*}
\rho_i = \min(\bar{\rho}, \frac{\pi(a_i|x_i)}{\mu(a_i|x_i)})
\end{equation*}

When $\bar{\rho}$ is infinite, then this is the value function $V^{pi}$
of the target policy. At the limite when $\bar{\rho}$ is close to zero,
we obtain the value function of the behaviour policy $V^{\mu}$.

The final V-trace is
\begin{array}
v_s &= V(x_s) + \sum_{t=s}^{s+n-1}\gamma^{t-s} \prod_{i=s}^{t-1}\frac{\pi(a_i|x_i)}{\mu(a_i|x_i)} (r_t + \gamma V(x_{t+1}) - V(x_t)) \\
\end{array}


In conclusion, $\bar{\rho}$ impacts the nature of the value function we
converge to(target policy or behaviour policy), whereas $\bar{c}$ impacts
the speed at which we convergence to this function.

** Policy gradient
\begin{equation}
\mathbb{E}_{a_{s} \sim \mu\left(\cdot \mid x_{s}\right)}\left[\frac{\pi_{\bar{\rho}}\left(a_{s} \mid x_{s}\right)}{\mu\left(a_{s} \mid x_{s}\right)} \nabla \log \pi_{\bar{\rho}}\left(a_{s} \mid x_{s}\right) q_{s} \mid x_{s}\right]
\end{equation}

where $q_s = r_s + \gamma v_{s+1}$.

** Actor-Critic
*** Critic update:
\begin{equation}
\left(v_{s}-V_{\theta}\left(x_{s}\right)\right) \nabla_{\theta} V_{\theta}\left(x_{s}\right)
\end{equation}

*** Actor update:
\begin{equation}
\rho_{s} \nabla_{\omega} \log \pi_{\omega}\left(a_{s} \mid x_{s}\right)\left(r_{s}+\gamma v_{s+1}-V_{\theta}\left(x_{s}\right)\right)
\end{equation}
with an entropy bonus:
\begin{equation}
-\nabla_{\omega} \sum_{a} \pi_{\omega}\left(a \mid x_{s}\right) \log \pi_{\omega}\left(a \mid x_{s}\right)
\end{equation}

* Reference
- https://zhuanlan.zhihu.com/p/58226117
- https://zhuanlan.zhihu.com/p/34074929
